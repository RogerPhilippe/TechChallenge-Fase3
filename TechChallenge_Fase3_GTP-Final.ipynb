{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RTNxpMRj-C2"
      },
      "source": [
        "* TECH CHALLENGE - FASE 03: FINE-TUNING AMAZON PRODUCTS\n",
        "* Vers√£o Simplificada com OpenAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-y2GIi5kYJ0"
      },
      "source": [
        "* 1: INSTALA√á√ÉO E CONFIGURA√á√ÉO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jQ6saeoAeTIP"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai pandas gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "19L5OGc_epG_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbYeD446evcx"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3JwVJtBe7n6",
        "outputId": "e4e700c8-daf1-4941-fdda-53c131db156b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conex√£o com OpenAI estabelecida!\n",
            "Modelos dispon√≠veis para fine-tuning: ['gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'gpt-3.5-turbo-instruct-0914']\n"
          ]
        }
      ],
      "source": [
        "# Testar conex√£o\n",
        "try:\n",
        "    models = client.models.list()\n",
        "    print(\"Conex√£o com OpenAI estabelecida!\")\n",
        "    print(\"Modelos dispon√≠veis para fine-tuning:\", [m.id for m in models.data if 'gpt-3.5-turbo' in m.id][:3])\n",
        "except Exception as e:\n",
        "    print(f\"Erro na conex√£o: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq6-U8QCkm1S"
      },
      "source": [
        "* 2: UPLOAD E AN√ÅLISE DO DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv3cr5QDkshJ"
      },
      "outputs": [],
      "source": [
        "!gunzip trn.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFg9WGq9pmFo",
        "outputId": "4e6fe030-7dc6-4bfc-fc51-bb43f4bf0961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset encontrado!\n"
          ]
        }
      ],
      "source": [
        "# Verificar se arquivo existe ou criar exemplo\n",
        "try:\n",
        "    with open('/content/trn.json', 'r') as f:\n",
        "        test_read = f.read(1)\n",
        "    print(\"Dataset encontrado!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nArquivo n√£o existe!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZV896M7pwRQ"
      },
      "outputs": [],
      "source": [
        "# Analisar dataset\n",
        "def analyze_dataset():\n",
        "    print(\"\\nAN√ÅLISE DO DATASET:\")\n",
        "\n",
        "    data = []\n",
        "    with open('/content/trn.json', 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= 10:  # Analisar primeiros 10\n",
        "                break\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                data.append(item)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    if data:\n",
        "        print(f\"{len(data)} exemplos carregados\")\n",
        "        print(f\"Campos: {list(data[0].keys())}\")\n",
        "\n",
        "        # Mostrar exemplo\n",
        "        print(f\"\\nExemplo:\")\n",
        "        print(f\"T√≠tulo: {data[0]['title']}\")\n",
        "        print(f\"Descri√ß√£o: {data[0]['content'][:100]}...\")\n",
        "\n",
        "        return data\n",
        "    else:\n",
        "        print(\"Erro ao carregar dados\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fkpr_LCp64i",
        "outputId": "2494678d-8574-44ad-f243-12c94be50a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "AN√ÅLISE DO DATASET:\n",
            "10 exemplos carregados\n",
            "Campos: ['uid', 'title', 'content', 'target_ind', 'target_rel']\n",
            "\n",
            "Exemplo:\n",
            "T√≠tulo: Girls Ballet Tutu Neon Pink\n",
            "Descri√ß√£o: High quality 3 layer ballet tutu. 12 inches in length...\n"
          ]
        }
      ],
      "source": [
        "sample_data = analyze_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4d8vWWBqD6J"
      },
      "source": [
        "* 3: PREPARA√á√ÉO DOS DADOS PARA FINE-TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l0yMvpHqE2A"
      },
      "outputs": [],
      "source": [
        "def prepare_openai_dataset(max_examples=50):\n",
        "    \"\"\"Prepara dados no formato da OpenAI para fine-tuning\"\"\"\n",
        "\n",
        "    print(f\"Preparando {max_examples} exemplos para fine-tuning...\")\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    with open('/content/trn.json', 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_examples:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                title = item.get('title', '').strip()\n",
        "                content = item.get('content', '').strip()\n",
        "\n",
        "                if not title or not content:\n",
        "                    continue\n",
        "\n",
        "                # Criar exemplos de treinamento\n",
        "                questions = [\n",
        "                    f\"Me fale sobre: {title}\",\n",
        "                    f\"Quais as caracter√≠sticas de: {title}\",\n",
        "                    f\"Descreva este produto: {title}\"\n",
        "                ]\n",
        "\n",
        "                for question in questions[:2]:  # 2 exemplos por produto\n",
        "                    training_example = {\n",
        "                        \"messages\": [\n",
        "                            {\n",
        "                                \"role\": \"system\",\n",
        "                                \"content\": \"Voc√™ √© um assistente especializado em produtos da Amazon. Forne√ßa informa√ß√µes detalhadas sobre produtos baseado nas descri√ß√µes fornecidas.\"\n",
        "                            },\n",
        "                            {\n",
        "                                \"role\": \"user\",\n",
        "                                \"content\": question\n",
        "                            },\n",
        "                            {\n",
        "                                \"role\": \"assistant\",\n",
        "                                \"content\": content\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                    training_data.append(training_example)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    print(f\"{len(training_data)} exemplos preparados\")\n",
        "\n",
        "    # Salvar em formato JSONL para OpenAI\n",
        "    with open('/content/training_data.jsonl', 'w', encoding='utf-8') as f:\n",
        "        for example in training_data:\n",
        "            f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(\"Dados salvos em '/content/training_data.jsonl'\")\n",
        "    return training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBcT8sPsqcwE",
        "outputId": "70745429-25eb-458d-a142-44386cb7fe8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparando 25 exemplos para fine-tuning...\n",
            "22 exemplos preparados\n",
            "Dados salvos em '/content/training_data.jsonl'\n",
            "\n",
            "Exemplo de dados preparados:\n",
            "{\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"role\": \"system\",\n",
            "      \"content\": \"Voc√™ √© um assistente especializado em produtos da Amazon. Forne√ßa informa√ß√µes detalhadas sobre produtos baseado nas descri√ß√µes fornecidas.\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"Me fale sobre: Girls Ballet Tutu Neon Pink\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": \"High quality 3 layer ballet tutu. 12 inches in length\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Preparar dados\n",
        "training_examples = prepare_openai_dataset(25)  # 25 produtos = ~50 exemplos\n",
        "\n",
        "# Mostrar exemplo\n",
        "print(\"\\nExemplo de dados preparados:\")\n",
        "print(json.dumps(training_examples[0], indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtpqY6OsqmSh"
      },
      "source": [
        "* 4: TESTE DO MODELO BASE (ANTES DO FINE-TUNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGa7Y1Unqm3n"
      },
      "outputs": [],
      "source": [
        "def test_base_model():\n",
        "    \"\"\"Testa GPT-3.5 sem fine-tuning\"\"\"\n",
        "\n",
        "    print(\"TESTE DO MODELO BASE (SEM FINE-TUNING)\")\n",
        "\n",
        "    test_questions = [\n",
        "        \"Me fale sobre fones de ouvido Sony WH-1000XM4\",\n",
        "        \"Quais as caracter√≠sticas do Samsung Galaxy S21 Ultra\",\n",
        "        \"Descreva o MacBook Air com M1\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\nPergunta: {question}\")\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente de produtos.\"},\n",
        "                    {\"role\": \"user\", \"content\": question}\n",
        "                ],\n",
        "                max_tokens=150,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            answer = response.choices[0].message.content\n",
        "            print(f\"Resposta BASE: {answer}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhK0fnJnr58k",
        "outputId": "ea15fc52-e275-4ad4-88c2-4dba00059376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TESTE DO MODELO BASE (SEM FINE-TUNING)\n",
            "\n",
            "Pergunta: Me fale sobre fones de ouvido Sony WH-1000XM4\n",
            "Resposta BASE: Os fones de ouvido Sony WH-1000XM4 s√£o conhecidos por sua excelente qualidade de som e cancelamento de ru√≠do l√≠der de mercado. Eles oferecem uma experi√™ncia de √°udio imersiva, com drivers de 40mm que proporcionam graves profundos e agudos cristalinos. \n",
            "\n",
            "Al√©m disso, esses fones possuem tecnologia de cancelamento de ru√≠do adapt√°vel, que ajusta automaticamente a intensidade do cancelamento de acordo com o ambiente ao seu redor. Isso garante uma experi√™ncia auditiva tranquila, seja em um ambiente movimentado ou em um local mais silencioso.\n",
            "\n",
            "Os Sony WH-1000XM4 tamb√©m contam\n",
            "\n",
            "Pergunta: Quais as caracter√≠sticas do Samsung Galaxy S21 Ultra\n",
            "Resposta BASE: O Samsung Galaxy S21 Ultra √© um smartphone topo de gama da Samsung, lan√ßado em 2021. Algumas das caracter√≠sticas principais deste modelo incluem:\n",
            "\n",
            "- Tela: Dynamic AMOLED de 6,8 polegadas com resolu√ß√£o de 3200 x 1440 pixels e taxa de atualiza√ß√£o de at√© 120Hz\n",
            "- Processador: Exynos 2100 ou Snapdragon 888 (dependendo da regi√£o)\n",
            "- Mem√≥ria RAM: 12GB ou 16GB\n",
            "- Armazenamento interno: 128GB, 256GB ou 512GB\n",
            "- C√¢meras traseiras: Sistema de c√¢mera qu√°drupla com sensor principal de \n",
            "\n",
            "Pergunta: Descreva o MacBook Air com M1\n",
            "Resposta BASE: O MacBook Air com M1 √© um laptop da Apple que possui um processador M1, desenvolvido pela pr√≥pria empresa. Este processador oferece um desempenho poderoso, com excelente efici√™ncia energ√©tica. O MacBook Air √© conhecido por ser leve e fino, tornando-o ideal para quem precisa de mobilidade sem comprometer a performance. Al√©m disso, ele possui uma tela Retina de alta resolu√ß√£o, teclado confort√°vel, Touch ID para seguran√ßa e v√°rias op√ß√µes de armazenamento. Em resumo, o MacBook Air com M1 √© uma excelente op√ß√£o para quem busca um laptop potente, leve e com uma √≥tima dura√ß√£o de bateria.\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "test_base_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0V2mDYpsDTv"
      },
      "source": [
        "* 5: UPLOAD DOS DADOS E FINE-TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weLCFFlUsESG"
      },
      "outputs": [],
      "source": [
        "def upload_and_finetune():\n",
        "    \"\"\"Faz upload dos dados e executa fine-tuning na OpenAI\"\"\"\n",
        "\n",
        "    print(\"INICIANDO PROCESSO DE FINE-TUNING\")\n",
        "\n",
        "    try:\n",
        "        # 1. Upload do arquivo de treinamento\n",
        "        print(\"Fazendo upload dos dados de treinamento...\")\n",
        "\n",
        "        with open('/content/training_data.jsonl', 'rb') as f:\n",
        "            training_file = client.files.create(\n",
        "                file=f,\n",
        "                purpose='fine-tune'\n",
        "            )\n",
        "\n",
        "        print(f\"Arquivo enviado! ID: {training_file.id}\")\n",
        "\n",
        "        # 2. Criar job de fine-tuning\n",
        "        print(\"Criando job de fine-tuning...\")\n",
        "\n",
        "        fine_tune_job = client.fine_tuning.jobs.create(\n",
        "            training_file=training_file.id,\n",
        "            model=\"gpt-3.5-turbo-1106\",  # Modelo base para fine-tuning\n",
        "            hyperparameters={\n",
        "                \"n_epochs\": 3,  # N√∫mero de √©pocas\n",
        "                \"batch_size\": 1,\n",
        "                \"learning_rate_multiplier\": 0.1\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(f\"Job criado! ID: {fine_tune_job.id}\")\n",
        "        print(f\"Status: {fine_tune_job.status}\")\n",
        "\n",
        "        # Salvar IDs importantes\n",
        "        job_info = {\n",
        "            \"training_file_id\": training_file.id,\n",
        "            \"job_id\": fine_tune_job.id,\n",
        "            \"status\": fine_tune_job.status\n",
        "        }\n",
        "\n",
        "        with open('/content/finetune_info.json', 'w') as f:\n",
        "            json.dump(job_info, f, indent=2)\n",
        "\n",
        "        print(\"Informa√ß√µes salvas em '/content/finetune_info.json'\")\n",
        "        return fine_tune_job\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro no fine-tuning: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_utJXumRsMNZ",
        "outputId": "55b01de9-f97a-4c4f-8371-5b6e63faa8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INICIANDO PROCESSO DE FINE-TUNING\n",
            "Fazendo upload dos dados de treinamento...\n",
            "Arquivo enviado! ID: file-P8CfbxPRChTrLj4uvPUiwC\n",
            "Criando job de fine-tuning...\n",
            "Job criado! ID: ftjob-g6uQNtY3iHSWBSZhBuCdeU3a\n",
            "Status: validating_files\n",
            "Informa√ß√µes salvas em '/content/finetune_info.json'\n"
          ]
        }
      ],
      "source": [
        "finetune_job = upload_and_finetune()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5hOYp7s51r"
      },
      "source": [
        "* 6: MONITORAR PROGRESSO DO FINE-TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G48ZIbjNs6kN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def monitor_finetune_progress():\n",
        "    \"\"\"Monitora o progresso do fine-tuning automaticamente com progress bar\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Carregar informa√ß√µes do job\n",
        "        with open('/content/finetune_info.json', 'r') as f:\n",
        "            job_info = json.load(f)\n",
        "\n",
        "        job_id = job_info['job_id']\n",
        "        print(f\"Monitorando job: {job_id}\")\n",
        "        print(\"Verificando status automaticamente...\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Verificar status\n",
        "                job = client.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "                # Limpar output anterior\n",
        "                clear_output(wait=True)\n",
        "\n",
        "                # Mostrar status atual\n",
        "                print(\"=\" * 60)\n",
        "                print(\"MONITORAMENTO AUTOM√ÅTICO DO FINE-TUNING\")\n",
        "                print(\"=\" * 60)\n",
        "                print(f\"Job ID: {job.id}\")\n",
        "                print(f\"Status: {job.status}\")\n",
        "                print(f\"Modelo base: {job.model}\")\n",
        "\n",
        "                # Progress bar visual\n",
        "                if job.status == \"running\":\n",
        "                    print(\"\\nProcessando... ‚è≥\")\n",
        "                    print(\"‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 75% (estimado)\")\n",
        "                elif job.status == \"validating\":\n",
        "                    print(\"\\nValidando modelo... üîç\")\n",
        "                    print(\"‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 95% (estimado)\")\n",
        "                elif job.status == \"succeeded\":\n",
        "                    print(\"\\nConclu√≠do! ‚úÖ\")\n",
        "                    print(\"‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%\")\n",
        "                elif job.status == \"failed\":\n",
        "                    print(\"\\nFalhou! ‚ùå\")\n",
        "                    print(\"Error:\", job.error)\n",
        "                    break\n",
        "\n",
        "                # Se terminou com sucesso\n",
        "                if job.finished_at and job.status == \"succeeded\":\n",
        "                    print(f\"\\nConclu√≠do em: {job.finished_at}\")\n",
        "                    print(f\"Modelo fine-tuned: {job.fine_tuned_model}\")\n",
        "\n",
        "                    # Salvar modelo ID\n",
        "                    job_info['fine_tuned_model'] = job.fine_tuned_model\n",
        "                    job_info['status'] = 'succeeded'\n",
        "                    with open('/content/finetune_info.json', 'w') as f:\n",
        "                        json.dump(job_info, f, indent=2)\n",
        "\n",
        "                    print(\"\\nFINE-TUNING CONCLU√çDO COM SUCESSO!\")\n",
        "                    print(\"Agora voc√™ pode executar a pr√≥xima c√©lula.\")\n",
        "                    break\n",
        "\n",
        "                # Se falhou\n",
        "                elif job.status == 'failed':\n",
        "                    print(f\"\\nFalhou: {job.error}\")\n",
        "                    job_info['status'] = 'failed'\n",
        "                    with open('/content/finetune_info.json', 'w') as f:\n",
        "                        json.dump(job_info, f, indent=2)\n",
        "                    break\n",
        "\n",
        "                # Se ainda em progresso\n",
        "                else:\n",
        "                    current_time = datetime.now().strftime(\"%d/%m/%Y √†s %H:%M:%S\")\n",
        "                    print(f\"\\nStatus: {job.status}\")\n",
        "                    print(f\"√öltima verifica√ß√£o: {current_time}\")\n",
        "                    print(\"Verificando novamente em 30 segundos...\")\n",
        "                    print(\"\\n(Pressione Ctrl+C para parar o monitoramento)\")\n",
        "                    time.sleep(30)  # Esperar 30 segundos\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\n\\nMonitoramento interrompido pelo usu√°rio.\")\n",
        "                print(\"Execute esta c√©lula novamente para continuar monitorando.\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Erro na verifica√ß√£o: {e}\")\n",
        "                print(\"Tentando novamente em 30 segundos...\")\n",
        "                time.sleep(30)\n",
        "\n",
        "        return job\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao iniciar monitoramento: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dGavZSWtD04",
        "outputId": "02b84637-7869-48e4-d3ff-86d4e348fd53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MONITORAMENTO AUTOM√ÅTICO DO FINE-TUNING\n",
            "============================================================\n",
            "Job ID: ftjob-g6uQNtY3iHSWBSZhBuCdeU3a\n",
            "Status: succeeded\n",
            "Modelo base: gpt-3.5-turbo-1106\n",
            "\n",
            "Conclu√≠do! ‚úÖ\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%\n",
            "\n",
            "Conclu√≠do em: 1757341408\n",
            "Modelo fine-tuned: ft:gpt-3.5-turbo-1106:personal::CDX4FhCa\n",
            "\n",
            "FINE-TUNING CONCLU√çDO COM SUCESSO!\n",
            "Agora voc√™ pode executar a pr√≥xima c√©lula.\n"
          ]
        }
      ],
      "source": [
        "print(\"Iniciando monitoramento autom√°tico...\")\n",
        "final_job = monitor_finetune_progress()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNWzsr-QtOnb"
      },
      "source": [
        "* 7: TESTE DO MODELO FINE-TUNED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AL3RFgYhtNRL"
      },
      "outputs": [],
      "source": [
        "def test_finetuned_model():\n",
        "    \"\"\"Testa o modelo ap√≥s fine-tuning\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Carregar informa√ß√µes do modelo\n",
        "        with open('/content/finetune_info.json', 'r') as f:\n",
        "            job_info = json.load(f)\n",
        "\n",
        "        if 'fine_tuned_model' not in job_info:\n",
        "            print(\"Modelo ainda n√£o est√° pronto. Verifique o status primeiro.\")\n",
        "            return\n",
        "\n",
        "        model_id = job_info['fine_tuned_model']\n",
        "        print(f\"TESTE DO MODELO FINE-TUNED: {model_id}\")\n",
        "\n",
        "        test_questions = [\n",
        "            \"Me fale sobre fones de ouvido Sony WH-1000XM4\",\n",
        "            \"Quais as caracter√≠sticas do Samsung Galaxy S21 Ultra\",\n",
        "            \"Descreva o MacBook Air com M1\"\n",
        "        ]\n",
        "\n",
        "        for question in test_questions:\n",
        "            print(f\"\\nPergunta: {question}\")\n",
        "\n",
        "            # Teste com modelo fine-tuned\n",
        "            response_ft = client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente especializado em produtos da Amazon.\"},\n",
        "                    {\"role\": \"user\", \"content\": question}\n",
        "                ],\n",
        "                max_tokens=150,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            answer_ft = response_ft.choices[0].message.content\n",
        "            print(f\"Resposta FINE-TUNED: {answer_ft}\")\n",
        "\n",
        "            # Comparar com modelo base\n",
        "            response_base = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente de produtos.\"},\n",
        "                    {\"role\": \"user\", \"content\": question}\n",
        "                ],\n",
        "                max_tokens=150,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            answer_base = response_base.choices[0].message.content\n",
        "            print(f\"Resposta BASE: {answer_base}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro no teste: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg0KRaictYpA",
        "outputId": "5e30f8a8-543d-44bf-e318-8f9bc142e290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TESTE DO MODELO FINE-TUNED: ft:gpt-3.5-turbo-1106:personal::CDX4FhCa\n",
            "\n",
            "Pergunta: Me fale sobre fones de ouvido Sony WH-1000XM4\n",
            "Resposta FINE-TUNED: Os fones de ouvido Sony WH-1000XM4 s√£o conhecidos por oferecer uma excelente qualidade de som e cancelamento de ru√≠do. Eles v√™m com tecnologia de cancelamento de ru√≠do adapt√°vel, o que significa que eles podem ajustar automaticamente o n√≠vel de cancelamento de ru√≠do com base no ambiente em que voc√™ est√°. Al√©m disso, eles possuem uma longa dura√ß√£o da bateria, permitindo at√© 30 horas de reprodu√ß√£o com uma √∫nica carga. Os WH-1000XM4 tamb√©m oferecem conectividade Bluetooth e suporte para assistentes de voz, como a Alexa da Amazon. Com um design confort√°vel e controles intuitivos,\n",
            "Resposta BASE: Os fones de ouvido Sony WH-1000XM4 s√£o considerados uma das melhores op√ß√µes de fones com cancelamento de ru√≠do ativo dispon√≠veis no mercado. Eles oferecem uma excelente qualidade de som, conforto e tecnologia avan√ßada.\n",
            "\n",
            "Al√©m do cancelamento de ru√≠do, os WH-1000XM4 possuem uma bateria de longa dura√ß√£o, suporte para Bluetooth e NFC, e controles intuitivos de toque. Tamb√©m possuem microfones para chamadas telef√¥nicas com qualidade de √°udio cristalina.\n",
            "\n",
            "Outro destaque √© o sistema de intelig√™ncia artificial que ajusta automaticamente o cancelamento de ru√≠do de\n",
            "--------------------------------------------------\n",
            "\n",
            "Pergunta: Quais as caracter√≠sticas do Samsung Galaxy S21 Ultra\n",
            "Resposta FINE-TUNED: O Samsung Galaxy S21 Ultra √© um smartphone de alto desempenho que oferece uma s√©rie de caracter√≠sticas avan√ßadas. Aqui est√£o algumas das principais caracter√≠sticas do Galaxy S21 Ultra:\n",
            "\n",
            "1. Tela: O Galaxy S21 Ultra possui uma tela Dynamic AMOLED de 6,8 polegadas com resolu√ß√£o de 3200 x 1440 pixels, oferecendo uma experi√™ncia visual impressionante e cores vibrantes.\n",
            "\n",
            "2. C√¢meras: O dispositivo possui um sistema de c√¢mera quad, com uma c√¢mera principal de 108 MP, uma lente ultra-wide de 12 MP, duas lentes telefoto de 10 MP cada, e uma c√¢mera frontal\n",
            "Resposta BASE: O Samsung Galaxy S21 Ultra √© um smartphone topo de gama da Samsung, lan√ßado em janeiro de 2021. Algumas das principais caracter√≠sticas do Galaxy S21 Ultra incluem:\n",
            "\n",
            "- Tela: Super AMOLED de 6,8 polegadas com resolu√ß√£o de 3200 x 1440 pixels e taxa de atualiza√ß√£o de at√© 120Hz\n",
            "- Processador: Exynos 2100 (em algumas regi√µes) ou Qualcomm Snapdragon 888 (em outras regi√µes)\n",
            "- Mem√≥ria RAM: 12GB ou 16GB\n",
            "- Armazenamento interno: 128GB, 256GB ou 512GB\n",
            "- C√¢meras traseiras: Sistema\n",
            "--------------------------------------------------\n",
            "\n",
            "Pergunta: Descreva o MacBook Air com M1\n",
            "Resposta FINE-TUNED: O MacBook Air com M1 √© um laptop fino, leve e poderoso que incorpora o processador M1, desenvolvido pela Apple. O processador M1 √© um chip de 8 n√∫cleos que oferece um desempenho excepcional em termos de velocidade e efici√™ncia energ√©tica. Isso significa que o MacBook Air com M1 oferece um desempenho r√°pido e responsivo, ao mesmo tempo em que possui uma excelente vida √∫til da bateria.\n",
            "\n",
            "Al√©m disso, o MacBook Air com M1 possui uma tela Retina de alta resolu√ß√£o, que oferece imagens n√≠tidas e cores vibrantes. O design elegante e compacto do MacBook\n",
            "Resposta BASE: O MacBook Air com M1 √© um laptop da Apple que foi lan√ßado em 2020. Ele √© alimentado pelo chip M1, desenvolvido pela pr√≥pria Apple, o que proporciona um desempenho r√°pido e eficiente. O MacBook Air com M1 possui uma tela Retina de alta resolu√ß√£o, design fino e leve, teclado confort√°vel, e uma bateria de longa dura√ß√£o. Al√©m disso, ele oferece um sistema operacional macOS otimizado para o chip M1, que proporciona uma experi√™ncia de uso fluida e eficiente. √â uma √≥tima op√ß√£o para quem busca um laptop potente e port√°til para tarefas do dia a\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Executar teste (s√≥ funciona depois do fine-tuning completar)\n",
        "test_finetuned_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gguJUHb32xTb"
      },
      "source": [
        "* 8: INTERFACE INTERATIVA COM GRADIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tJv3kj6u2wLh"
      },
      "outputs": [],
      "source": [
        "def create_chatbot_interface():\n",
        "    \"\"\"Cria interface Gradio para o chatbot\"\"\"\n",
        "\n",
        "    # Carregar modelo fine-tuned\n",
        "    try:\n",
        "        with open('/content/finetune_info.json', 'r') as f:\n",
        "            job_info = json.load(f)\n",
        "\n",
        "        if 'fine_tuned_model' in job_info:\n",
        "            custom_model_id = job_info['fine_tuned_model']\n",
        "            print(f\"Modelo personalizado dispon√≠vel: {custom_model_id}\")\n",
        "        else:\n",
        "            custom_model_id = None\n",
        "            print(\"Modelo personalizado n√£o encontrado\")\n",
        "\n",
        "    except:\n",
        "        custom_model_id = None\n",
        "        print(\"Arquivo de modelo n√£o encontrado\")\n",
        "\n",
        "    def generate_response(message, history, use_custom_model):\n",
        "        \"\"\"Gera resposta usando OpenAI\"\"\"\n",
        "\n",
        "        # Escolher modelo baseado na sele√ß√£o do usu√°rio\n",
        "        if use_custom_model and custom_model_id:\n",
        "            model_id = custom_model_id\n",
        "            system_message = \"Voc√™ √© um assistente especializado em produtos da Amazon. Forne√ßa informa√ß√µes detalhadas e √∫teis sobre produtos baseado no seu treinamento espec√≠fico.\"\n",
        "        else:\n",
        "            model_id = \"gpt-3.5-turbo\"\n",
        "            system_message = \"Voc√™ √© um assistente de produtos. Forne√ßa informa√ß√µes gerais sobre produtos.\"\n",
        "\n",
        "        # Construir contexto da conversa\n",
        "        messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "        # Adicionar hist√≥rico\n",
        "        for user_msg, assistant_msg in history:\n",
        "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "        # Adicionar mensagem atual\n",
        "        messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                messages=messages,\n",
        "                max_tokens=200,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            # Adicionar indicador do modelo usado na resposta\n",
        "            model_indicator = \" [PERSONALIZADO]\" if use_custom_model and custom_model_id else \" [PADR√ÉO]\"\n",
        "            answer = response.choices[0].message.content + model_indicator\n",
        "\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erro: {str(e)}\"\n",
        "\n",
        "    # Criar interface Gradio\n",
        "    with gr.Blocks(title=\"Amazon Product Assistant\") as demo:\n",
        "        gr.HTML(\"<h1>Assistente de Produtos Amazon - Compara√ß√£o de Modelos</h1>\")\n",
        "\n",
        "        # Seletor de modelo\n",
        "        with gr.Row():\n",
        "            model_selector = gr.Radio(\n",
        "                choices=[\n",
        "                    (\"Modelo Personalizado (Fine-tuned)\", True),\n",
        "                    (\"Modelo Padr√£o (GPT-3.5)\", False)\n",
        "                ],\n",
        "                value=True if custom_model_id else False,\n",
        "                label=\"Escolha o Modelo\",\n",
        "                interactive=True if custom_model_id else False\n",
        "            )\n",
        "\n",
        "        # Informa√ß√µes dos modelos\n",
        "        if custom_model_id:\n",
        "            gr.HTML(f\"<p><strong>Modelo Personalizado:</strong> <code>{custom_model_id}</code></p>\")\n",
        "        else:\n",
        "            gr.HTML(\"<p><strong>Aviso:</strong> Modelo personalizado n√£o dispon√≠vel. Usando apenas modelo padr√£o.</p>\")\n",
        "\n",
        "        gr.HTML(\"<p><strong>Modelo Padr√£o:</strong> <code>gpt-3.5-turbo</code></p>\")\n",
        "\n",
        "        chatbot = gr.Chatbot()\n",
        "\n",
        "        with gr.Row():\n",
        "            msg = gr.Textbox(\n",
        "                placeholder=\"Fa√ßa uma pergunta sobre produtos...\",\n",
        "                label=\"Sua pergunta\",\n",
        "                scale=4\n",
        "            )\n",
        "            submit = gr.Button(\"Enviar\", scale=1)\n",
        "\n",
        "        clear = gr.Button(\"Limpar Chat\")\n",
        "\n",
        "        # Exemplos\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                \"Me fale sobre fones de ouvido Sony\",\n",
        "                \"Quais as caracter√≠sticas de smartphones Samsung?\",\n",
        "                \"Descreva produtos Apple\",\n",
        "                \"Preciso de informa√ß√µes sobre t√™nis Nike\",\n",
        "                \"O que voc√™ sabe sobre panelas el√©tricas?\"\n",
        "            ],\n",
        "            inputs=msg\n",
        "        )\n",
        "\n",
        "        # Informa√ß√µes sobre a compara√ß√£o\n",
        "        gr.HTML(\"\"\"\n",
        "        <div style=\"background-color: #f0f0f0; padding: 10px; margin-top: 20px; border-radius: 5px;\">\n",
        "        <h3>Como comparar os modelos:</h3>\n",
        "        <ul>\n",
        "        <li><strong>Modelo Personalizado:</strong> Treinado com produtos espec√≠ficos da Amazon, deve dar respostas mais detalhadas e espec√≠ficas</li>\n",
        "        <li><strong>Modelo Padr√£o:</strong> Conhecimento geral, respostas mais gen√©ricas sobre produtos</li>\n",
        "        <li><strong>Indicador:</strong> Cada resposta mostra [PERSONALIZADO] ou [PADR√ÉO] para identificar qual modelo foi usado</li>\n",
        "        </ul>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        def respond(message, chat_history, use_custom):\n",
        "            bot_message = generate_response(message, chat_history, use_custom)\n",
        "            chat_history.append((message, bot_message))\n",
        "            return \"\", chat_history\n",
        "\n",
        "        def clear_chat():\n",
        "            return []\n",
        "\n",
        "        msg.submit(respond, [msg, chatbot, model_selector], [msg, chatbot])\n",
        "        submit.click(respond, [msg, chatbot, model_selector], [msg, chatbot])\n",
        "        clear.click(clear_chat, outputs=[chatbot])\n",
        "\n",
        "    return demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "mimbff8n247v",
        "outputId": "15ee12d9-fcef-4d27-8c20-6f08294f5fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo personalizado dispon√≠vel: ft:gpt-3.5-turbo-1106:personal::CDX4FhCa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3389249057.py:83: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://36f47bb63c91fed5df.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://36f47bb63c91fed5df.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://36f47bb63c91fed5df.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Criar e executar interface\n",
        "demo = create_chatbot_interface()\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
