{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* TECH CHALLENGE - FASE 03: FINE-TUNING AMAZON PRODUCTS\n",
        "* Versão baseada no notebook do professor (Unsloth)"
      ],
      "metadata": {
        "id": "6WLIen6pItWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1: CONFIGURAÇÃO E INSTALAÇÃO DAS DEPENDÊNCIAS"
      ],
      "metadata": {
        "id": "5kFMEE8PI0G_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irTTFrpaIALU"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install transformers datasets gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "0cw17DezJco9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch versão: {torch.__version__}\")\n",
        "print(f\"CUDA disponível: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "tkyet-cSJXGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 2: PREPARAÇÃO DO DATASET AMAZON"
      ],
      "metadata": {
        "id": "Gh7Tpr_PJwNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip trn.json.gz"
      ],
      "metadata": {
        "id": "tVlVDW8HKqud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    with open('/content/trn.json', 'r') as f:\n",
        "        test_line = f.readline()\n",
        "    print(\"Dataset encontrado!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Erro ao tentar carregar o arquivo!\")"
      ],
      "metadata": {
        "id": "GQOcbzAYJwp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 3: FORMATAÇÃO DOS DADOS PARA FINE-TUNING"
      ],
      "metadata": {
        "id": "pjH9aSnIMxPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_amazon_dataset_expanded(json_path=\"/content/trn.json\", max_examples=150):\n",
        "\n",
        "    print(f\"Formatting {max_examples} fine-tuning products...\")\n",
        "\n",
        "    training_examples = []\n",
        "\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_examples:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                title = item.get('title', '').strip()\n",
        "                content = item.get('content', '').strip()\n",
        "\n",
        "                if not title or not content:\n",
        "                    continue\n",
        "\n",
        "                # MAIS variações de perguntas para melhor generalização\n",
        "                questions = [\n",
        "                    f\"Tell me about this product: {title}\",\n",
        "                    f\"What are the characteristics of: {title}\",\n",
        "                    f\"Describe in detail: {title}\",\n",
        "                    f\"What can you tell me about: {title}\",\n",
        "                    f\"I need information about: {title}\"\n",
        "                ]\n",
        "\n",
        "                # 3 exemplos por produto\n",
        "                for question in questions[:3]:\n",
        "                    example = {\n",
        "                        \"instruction\": \"DESCRIBE THIS AMAZON PRODUCT\",\n",
        "                        \"input\": question,\n",
        "                        \"output\": content\n",
        "                    }\n",
        "                    training_examples.append(example)\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "    print(f\"Expanded dataset: {len(training_examples)} created examples\")\n",
        "    return training_examples"
      ],
      "metadata": {
        "id": "i46DzYqkMvXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_training_data = format_amazon_dataset_expanded(max_examples=200)"
      ],
      "metadata": {
        "id": "_O3H2KmbNFDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/amazon_formatted_data_expanded.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(amazon_training_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Expanded data saved: {len(amazon_training_data)} examples\")\n",
        "print(f\"Example:\")\n",
        "print(json.dumps(amazon_training_data[0], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "tGNpBOwgNJbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 4: CONFIGURAÇÃO DO MODELO BASE"
      ],
      "metadata": {
        "id": "vWU1Q0LsNda9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "print(\"Loading Llama-3-8B model...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"Model loaded!\")\n",
        "print(f\"Parameters: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "FN6ga1YGNePX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 5: TESTE DO MODELO ANTES DO FINE-TUNING"
      ],
      "metadata": {
        "id": "sAsHLrZfOsmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Teste simples que o modelo base consegue processar\n",
        "test_input_simple = \"Describe Sony WH-1000XM4 headphones:\"\n",
        "\n",
        "# Modified to pass input as a list\n",
        "inputs = tokenizer([test_input_simple], return_tensors=\"pt\").to(\"cuda\") # return_tensors=\"pt\" pytorch\n",
        "\n",
        "print(\"Testing BASE model...\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(response.replace(\"<|begin_of_text|>\", \"\"))"
      ],
      "metadata": {
        "id": "9-r4uxNBO16g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 6: CONFIGURAÇÃO DO FINE-TUNING COM LoRA"
      ],
      "metadata": {
        "id": "SER2e-rOQL-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"LoRA configured!\")\n",
        "print(f\"Trainable parameters: {model.print_trainable_parameters()}\")"
      ],
      "metadata": {
        "id": "CDQGFrveO7-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 7: PREPARAÇÃO DOS DADOS PARA TREINAMENTO"
      ],
      "metadata": {
        "id": "tbDjM-ZsQUx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = Dataset.from_list(amazon_training_data)\n",
        "print(f\"Dataset: {len(dataset)} examples\")\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "print(\"Formatted dataset!\")"
      ],
      "metadata": {
        "id": "av4E_11tQVsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 8: CONFIGURAÇÃO E EXECUÇÃO DO TREINAMENTO"
      ],
      "metadata": {
        "id": "blvnAd7eQdR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Setting up expanded training...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=15,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Starting expanded training...\")"
      ],
      "metadata": {
        "id": "jYxI6cq8QeSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"Final Loss: {trainer_stats.training_loss:.4f}\")"
      ],
      "metadata": {
        "id": "JvK27qIzQkiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 9: TESTE DO MODELO APÓS FINE-TUNING"
      ],
      "metadata": {
        "id": "LZ9yclVAR348"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "test_questions = [\n",
        "    \"Describe Sony WH-1000XM4 headphones\",\n",
        "    \"What are the features of: Samsung Galaxy S21 Ultra\",\n",
        "    \"What can you tell me about: Nike Air Max 270\"\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"QUESTION: {question}\")\n",
        "\n",
        "    test_prompt = alpaca_prompt.format(\n",
        "        \"DESCRIBE THIS AMAZON PRODUCT\",\n",
        "        question,\n",
        "        \"\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    print(\"RESPONSE AFTER FINE-TUNING:\")\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        streamer=text_streamer,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(full_response)"
      ],
      "metadata": {
        "id": "Y3G2XTtcR4rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 10: SALVAR MODELO"
      ],
      "metadata": {
        "id": "uqy9RyxLIlP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Salvando modelo expandido...\")\n",
        "\n",
        "model.save_pretrained(\"amazon_product_model_expanded\")\n",
        "tokenizer.save_pretrained(\"amazon_product_model_expanded\")"
      ],
      "metadata": {
        "id": "92EqUISAIkMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "befWLI3b4Y4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 11: EXECUÇÃO MANUAL MODELO"
      ],
      "metadata": {
        "id": "kRHDkJf04cBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt = alpaca_prompt.format(\n",
        "    \"DESCRIBE THIS AMAZON PRODUCT\",\n",
        "    \"Tell me about: Nike Air Max 270\",\n",
        "    \"\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "print(\"RESPONSE AFTER FINE-TUNING:\")\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(full_response)"
      ],
      "metadata": {
        "id": "RjHF1UlW4XtG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}